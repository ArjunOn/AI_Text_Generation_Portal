Date: 2025-10-30
Run: Short sanity training (200 iters) using project venv
Working dir: A:/Projects/AI_Text_Generation_Portal/Modelling/nanoGPT_project
Command:
& 'A:\Projects\AI_Text_Generation_Portal\Modelling\nanoGPT_project\.venv\Scripts\python.exe' 'A:\Projects\AI_Text_Generation_Portal\Modelling\nanoGPT_project\train.py' 'A:\Projects\AI_Text_Generation_Portal\Modelling\nanoGPT_project\config\train_custom.py' --device=cpu --compile=False --eval_iters=10 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=2 --n_head=2 --n_embd=64 --max_iters=200 --lr_decay_iters=200 --dropout=0.0

Notes:
- Goal: short end-to-end sanity run to verify training pipeline and capture initial loss traces.
- Environment: used the project's `.venv` at `Modelling/nanoGPT_project\.venv`.

Terminal output (captured):

dropout=0.0                                                                                                            Overriding config with .\config\train_custom.py:
# custom training config for cleaned twitter corpus
out_dir = 'out-custom'
eval_interval = 250
eval_iters = 200
log_interval = 10

always_save_checkpoint = False

wandb_log = False
wandb_project = 'custom-gpt'
wandb_run_name = 'custom-gpt-run'

# set dataset folder name (folder under data/)
dataset = 'custom_corpus'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256

# small model for experimentation
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3
max_iters = 5000
lr_decay_iters = 5000
min_lr = 1e-4
beta2 = 0.99

warmup_iters = 100

Overriding: device = cpu
Overriding: compile = False
Overriding: eval_iters = 10
Overriding: log_interval = 1
Overriding: block_size = 64
Overriding: batch_size = 12
Overriding: n_layer = 2
Overriding: n_head = 2
Overriding: n_embd = 64
Overriding: max_iters = 200
Overriding: lr_decay_iters = 200
Overriding: dropout = 0.0
tokens per iteration will be: 768
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 3.32M
num decayed parameter tensors: 10, with 3,321,856 parameters
num non-decayed parameter tensors: 5, with 320 parameters
using fused AdamW: False
step 0: train loss 10.8145, val loss 10.8242
iter 0: loss 10.8115, time 1477.08ms, mfu -100.00%
iter 1: loss 10.8320, time 208.14ms, mfu -100.00%
iter 2: loss 10.8169, time 209.16ms, mfu -100.00%
iter 3: loss 10.8115, time 167.21ms, mfu -100.00%
iter 4: loss 10.8146, time 155.59ms, mfu -100.00%
iter 5: loss 10.8090, time 154.58ms, mfu 0.03%
iter 6: loss 10.7971, time 148.65ms, mfu 0.03%
iter 7: loss 10.7842, time 152.35ms, mfu 0.03%
iter 8: loss 10.7888, time 149.94ms, mfu 0.03%
iter 9: loss 10.7962, time 160.31ms, mfu 0.03%
iter 10: loss 10.7746, time 154.10ms, mfu 0.03%
... (intermediate iterations omitted here for brevity) ...
iter 195: loss 7.1012, time 155.18ms, mfu 0.03%
iter 196: loss 7.1678, time 158.60ms, mfu 0.03%
iter 197: loss 7.0373, time 155.35ms, mfu 0.03%
iter 198: loss 7.1004, time 156.20ms, mfu 0.03%
iter 199: loss 7.3157, time 154.33ms, mfu 0.03%
iter 200: loss 6.9179, time 158.64ms, mfu 0.03%

Summary observations:
- Run completed: 200 iterations.
- Loss trend: initial train loss ~10.8 -> final ~6.92 (substantial decrease over the short run).
- Per-iteration runtime ~150-160ms on CPU in this environment.

Next steps (suggested):
- Review `out-custom` for checkpoints (if saved) and consider running `sample.py` once a checkpoint exists.
- Run a longer experiment or run small hyperparameter sweeps (increase n_layer/n_embd carefully for CPU), or switch to GPU if available for practical training speeds.
